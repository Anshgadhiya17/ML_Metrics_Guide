# ğŸ¤– Unsupervised Learning â€“ Complete Guide (With Models, Terms & Examples)

Unsupervised Learning is a type of Machine Learning where:

ğŸ‘‰ There is NO target/output column  
ğŸ‘‰ Model tries to find patterns in data by itself  

---

# ğŸ“Œ Supervised vs Unsupervised

| Feature | Supervised | Unsupervised |
|----------|------------|--------------|
| Target Variable | Yes | No |
| Example | Spam Detection | Customer Segmentation |
| Output | Predict value | Find hidden patterns |

---

# ğŸ¯ Where Unsupervised Learning is Used?

âœ” Customer Segmentation  
âœ” Market Basket Analysis  
âœ” Anomaly Detection  
âœ” Data Compression  
âœ” Pattern Recognition  
âœ” Recommendation Systems  

---

# ğŸ”¹ Types of Unsupervised Learning

1ï¸âƒ£ Clustering  
2ï¸âƒ£ Association Rule Learning  
3ï¸âƒ£ Dimensionality Reduction  
4ï¸âƒ£ Anomaly Detection  

---

# ğŸ§© 1ï¸âƒ£ Clustering

Clustering means grouping similar data points together.

Example:
Group customers based on:
- Age
- Income
- Spending score

---

## Important Terms in Clustering

### ğŸ”¹ Cluster
Group of similar data points.

### ğŸ”¹ Centroid
Center point of a cluster.

In K-Means:
Centroid = Mean of all points in that cluster.

### ğŸ”¹ Distance Metric
Used to measure similarity.

Common distances:
- Euclidean Distance
- Manhattan Distance

### ğŸ”¹ Inertia (WCSS)
Within Cluster Sum of Squares.
Measures how tightly data points are grouped.

Lower inertia = Better clustering.

---

# ğŸ“Š K-Means Clustering

Most popular clustering algorithm.

## How It Works:

1. Choose K (number of clusters)
2. Randomly initialize K centroids
3. Assign each point to nearest centroid
4. Recalculate centroids
5. Repeat until centroids stop changing

---

## Example Code

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

X = np.array([[1,2],[1,4],[1,0],
              [10,2],[10,4],[10,0]])

kmeans = KMeans(n_clusters=2)
kmeans.fit(X)

print(kmeans.cluster_centers_)
print(kmeans.labels_)
```

---

## Choosing K (Elbow Method)

```python
inertia = []

for k in range(1, 10):
    model = KMeans(n_clusters=k)
    model.fit(X)
    inertia.append(model.inertia_)
```

Plot inertia vs K â†’ Choose elbow point.

---

# ğŸ“Š Hierarchical Clustering

Builds cluster tree (Dendrogram).

Two Types:
- Agglomerative (Bottom-Up)
- Divisive (Top-Down)

---

## Dendrogram

Tree-like diagram showing cluster merging.

```python
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X, method='ward')
dendrogram(linked)
```

---

# ğŸ“Š DBSCAN (Density-Based Clustering)

Density-Based Spatial Clustering.

Groups points that are close together.

Good for:
âœ” Noise detection  
âœ” Arbitrary shaped clusters  

Important Parameters:
- eps (distance radius)
- min_samples (minimum points)

---

## Example

```python
from sklearn.cluster import DBSCAN

model = DBSCAN(eps=3, min_samples=2)
model.fit(X)

print(model.labels_)
```

---

# ğŸ“‰ 2ï¸âƒ£ Dimensionality Reduction

Used when dataset has too many features.

Goal:
Reduce features but keep important information.

---

## PCA (Principal Component Analysis)

Transforms data into fewer dimensions.

Important Terms:

### ğŸ”¹ Principal Component
New feature created from original features.

### ğŸ”¹ Variance
Amount of information retained.

---

## Example

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

print(pca.explained_variance_ratio_)
```

---

# ğŸ“¦ 3ï¸âƒ£ Association Rule Learning

Used in Market Basket Analysis.

Example:
People who buy bread also buy butter.

---

## Important Terms

### ğŸ”¹ Support
How frequently item appears.

### ğŸ”¹ Confidence
Probability of buying Y given X.

### ğŸ”¹ Lift
Strength of rule.
Lift > 1 means strong relationship.

---

## Apriori Algorithm

Used to generate association rules.

Example:

```python
from mlxtend.frequent_patterns import apriori
```

---

# ğŸš¨ 4ï¸âƒ£ Anomaly Detection

Detect unusual data points.

Used in:
âœ” Fraud detection  
âœ” Network security  
âœ” Fault detection  

Algorithms:
- Isolation Forest
- One-Class SVM

---

# ğŸ“Œ Isolation Forest Example

```python
from sklearn.ensemble import IsolationForest

model = IsolationForest()
model.fit(X)

predictions = model.predict(X)
```

Output:
1 â†’ Normal  
-1 â†’ Anomaly  

---

# ğŸ“Š Evaluation in Unsupervised Learning

Since no labels:

We use:

âœ” Silhouette Score  
âœ” Inertia  
âœ” Davies-Bouldin Score  

---

## Silhouette Score

Range: -1 to 1

Higher = Better clustering

```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X, kmeans.labels_)
print(score)
```

---

# ğŸ“˜ Important Terms Summary

| Term | Meaning |
|------|---------|
| Cluster | Group of similar data |
| Centroid | Center of cluster |
| Inertia | Compactness measure |
| Dendrogram | Tree of clusters |
| Principal Component | New reduced feature |
| Support | Frequency of item |
| Confidence | Conditional probability |
| Lift | Strength of rule |
| Outlier | Unusual data point |

---

# ğŸ”¥ When to Use Which Algorithm?

| Problem | Algorithm |
|----------|------------|
| Simple clustering | K-Means |
| Hierarchy needed | Hierarchical |
| Noise present | DBSCAN |
| Reduce features | PCA |
| Market basket | Apriori |
| Fraud detection | Isolation Forest |

---


# ğŸš€ Final Summary

âœ” Unsupervised learning finds hidden patterns  
âœ” No target variable  
âœ” K-Means most common  
âœ” PCA for dimensionality reduction  
âœ” DBSCAN for noise handling  
âœ” Association rules for shopping analysis  

Unsupervised Learning = Discover patterns without labels
