# ğŸ“˜ Supervised Learning â€“ Complete Guide (With Models, Terms & Examples)

Supervised Learning is a type of Machine Learning where:

ğŸ‘‰ We have input data (X)  
ğŸ‘‰ We have output/target labels (y)  
ğŸ‘‰ Model learns mapping from X â†’ y  

It learns from labeled data.

---

# ğŸ“Œ Example of Supervised Learning

âœ” Predict house price  
âœ” Spam detection  
âœ” Disease prediction  
âœ” Student pass/fail  
âœ” Sales forecasting  

---

# ğŸ§  Types of Supervised Learning

1ï¸âƒ£ Regression  
2ï¸âƒ£ Classification  

---

# ğŸ”¹ 1ï¸âƒ£ Regression

Used when output is continuous numeric value.

Examples:
- House price prediction
- Salary prediction
- Temperature prediction

---

# ğŸ”¹ 2ï¸âƒ£ Classification

Used when output is category/class.

Examples:
- Spam or Not Spam
- Yes or No
- Cat, Dog, Bird

---

# ğŸ“Š Important Terms in Supervised Learning

## ğŸ”¹ Feature (Independent Variable)
Input variables.

Example:
Area, bedrooms â†’ house price model

---

## ğŸ”¹ Target (Dependent Variable)
Output variable.

Example:
House price

---

## ğŸ”¹ Training Data
Data used to train model.

---

## ğŸ”¹ Testing Data
Data used to evaluate model.

---

## ğŸ”¹ Overfitting
Model performs very well on training data  
But poor on testing data.

---

## ğŸ”¹ Underfitting
Model performs poorly on both training & testing data.

---

## ğŸ”¹ Bias
Error due to wrong assumptions.

---

## ğŸ”¹ Variance
Error due to model complexity.

---

## ğŸ”¹ Loss Function
Measures how wrong predictions are.

---

## ğŸ”¹ Accuracy
Percentage of correct predictions.

---

# ğŸ“ˆ Regression Models

---

# ğŸ”¹ Linear Regression

Simple linear relationship between X and y.

Equation:
y = mx + b

Example:

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

prediction = model.predict(X_test)
```

Use when:
âœ” Data is linear  
âœ” Simple problem  

---

# ğŸ”¹ Polynomial Regression

Used when relationship is curved.

---

# ğŸ”¹ Decision Tree Regressor

Splits data based on conditions.

Good for:
âœ” Non-linear data  
âœ” Easy interpretation  

---

# ğŸ”¹ Random Forest Regressor

Collection of multiple decision trees.

Advantages:
âœ” High accuracy  
âœ” Reduces overfitting  

---

# ğŸ“Š Classification Models

---

# ğŸ”¹ Logistic Regression

Used for binary classification.

Output between 0 and 1 (probability).

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
```

---

# ğŸ”¹ K-Nearest Neighbors (KNN)

Classifies based on nearest neighbors.

Parameter:
K = Number of neighbors

---

# ğŸ”¹ Decision Tree Classifier

Tree-based classification.

---

# ğŸ”¹ Random Forest Classifier

Multiple decision trees combined.

Very powerful & commonly used.

---

# ğŸ”¹ Support Vector Machine (SVM)

Finds best boundary (hyperplane).

Works well in:
âœ” High dimensional data  
âœ” Text classification  

---

# ğŸ”¹ Naive Bayes

Based on probability theorem.

Used in:
âœ” Spam detection  
âœ” Text classification  

---

# ğŸ“Š Evaluation Metrics (Regression)

| Metric | Meaning |
|---------|----------|
| MAE | Mean Absolute Error |
| MSE | Mean Squared Error |
| RMSE | Root Mean Squared Error |
| RÂ² Score | Goodness of fit |

Example:

```python
from sklearn.metrics import mean_squared_error
```

---

# ğŸ“Š Evaluation Metrics (Classification)

| Metric | Meaning |
|---------|----------|
| Accuracy | Correct predictions |
| Precision | True Positive / Predicted Positive |
| Recall | True Positive / Actual Positive |
| F1-Score | Harmonic mean of Precision & Recall |
| Confusion Matrix | Detailed result table |

---

# ğŸ“‰ Confusion Matrix

| Actual \ Predicted | Positive | Negative |
|---------------------|----------|----------|
| Positive | TP | FN |
| Negative | FP | TN |

---

# ğŸ“Š Bias-Variance Tradeoff

High Bias â†’ Underfitting  
High Variance â†’ Overfitting  

Goal:
Find balance between both.

---

# ğŸ“ˆ Model Training Process

1. Collect Data  
2. Clean Data  
3. Split Data (Train/Test)  
4. Train Model  
5. Evaluate Model  
6. Tune Hyperparameters  
7. Deploy Model  

---

# ğŸ“Œ Cross Validation

Used to evaluate model better.

Example:

```python
from sklearn.model_selection import cross_val_score
```

---

# ğŸ“Œ Hyperparameters

Parameters set before training.

Examples:
- K in KNN
- Depth in Decision Tree
- Learning rate

---

# ğŸ“Š Supervised vs Other Learning

| Feature | Supervised | Unsupervised | Reinforcement |
|----------|------------|--------------|---------------|
| Labels | Yes | No | No |
| Output | Predict value | Find patterns | Reward-based |
| Example | Spam detection | Clustering | Game AI |

---

# ğŸ¯ When to Use Which Model?

| Problem | Recommended Model |
|----------|-------------------|
| Linear data | Linear Regression |
| Non-linear data | Random Forest |
| Binary classification | Logistic Regression |
| High accuracy needed | Random Forest |
| Text data | Naive Bayes / SVM |
| Small dataset | KNN |

---

# ğŸš€ Final Summary

âœ” Supervised learning uses labeled data  
âœ” Two types: Regression & Classification  
âœ” Many algorithms available  
âœ” Need evaluation metrics  
âœ” Avoid overfitting  
âœ” Tune hyperparameters  

Supervised Learning = Learn from labeled examples
